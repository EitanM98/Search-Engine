{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"a00e032c"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"51cf86c5"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":2,"id":"bf199e6a","metadata":{"id":"bf199e6a","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')\n"]},{"cell_type":"code","execution_count":3,"id":"47900073","metadata":{"id":"47900073","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":4,"id":"980e62a5","metadata":{"id":"980e62a5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'mybucketbucket' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if \"parquet\" in b.name:\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"cac891c2","metadata":{"id":"cac891c2"},"source":["***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"582c3f5e"},"source":["# Building an inverted index"]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"481f2044"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"code","execution_count":9,"id":"e4c523e7","metadata":{"id":"e4c523e7","outputId":"76cfc76a-59a6-4ab7-f993-6b6d58fe7445","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n"]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"0d7e2971"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"code","execution_count":6,"id":"82881fbf","metadata":{"id":"82881fbf"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["6348910"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"code","execution_count":11,"id":"121fe102","metadata":{"id":"121fe102","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":12,"id":"57c101a8","metadata":{"id":"57c101a8","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["23/01/05 16:35:32 WARN org.apache.spark.SparkContext: The path /home/dataproc/inverted_index_gcp.py has been added already. Overwriting of added paths is not supported in the current version.\n"]}],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":13,"id":"c259c402","metadata":{"id":"c259c402"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"b017d056","metadata":{},"source":["<h1>Doc Id - Title dictionary</h1>\n"]},{"cell_type":"code","execution_count":null,"id":"d2a2b9ea","metadata":{},"outputs":[],"source":["# doc_id_title_dict creation\n","doc_id_title_pairs = parquetFile.select(\"id\", \"title\").rdd\n","doc_id_title_dict = doc_id_title_pairs.collectAsMap()\n","\n","with open('doc_id_title_dict.pickle', 'wb') as f:\n","    pickle.dump(doc_id_title_dict, f)\n","\n","index_src = \"doc_id_title_dict.pickle\"\n","index_dst = f'gs://{bucket_name}/indexes/{index_src}'\n","!gsutil cp $index_src $index_dst\n","\n","!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"353e36d8","metadata":{"id":"353e36d8"},"source":["<H1> Helper function to create Indexes <H1/>"]},{"cell_type":"code","execution_count":14,"id":"2addd40e","metadata":{},"outputs":[],"source":["index_type = \"_title\""]},{"cell_type":"code","execution_count":15,"id":"f3ad8fea","metadata":{"id":"f3ad8fea","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def word_count(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    cnt = Counter([tok for tok in tokens if tok not in all_stopwords])\n","    return [(key, (id, val)) for key,val in cnt.items()]\n","    \n","    \n","def reduce_word_counts(unsorted_pl):\n","    return sorted(unsorted_pl, key= lambda tup: tup[0])\n","\n","\n","def calculate_df(postings):\n","    return postings.map(lambda lm: (lm[0],len(lm[1])))\n","\n","\n","def partition_postings_and_write(postings):\n","    partition_rdd = postings.map(lambda w_pl: (token2bucket_id(w_pl[0]),w_pl)).groupByKey()\n","    return partition_rdd.map(lambda b_w_pl: InvertedIndex.write_a_posting_list(b_w_pl,bucket_name,index_type=index_type))\n","\n","def word_count_binary(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    words_set = set(tok for tok in tokens if tok not in all_stopwords)\n","    return [(word, (id, 1)) for word in words_set]\n","\n","def doc_len_count(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    return (id,len(tokens))\n","\n","# Assignment2 Code: \n","\n","def tokenize(text):\n","  return [token.group() for token in RE_WORD.finditer(text.lower())]\n","\n","def calculate_tf(docs):\n","  word_counts = Counter()\n","  for text in docs:\n","    tokens = tokenize(title)\n","    doc_counts = Counter(tokens)\n","    word_counts.update(doc_counts)\n","  return word_counts"]},{"cell_type":"markdown","id":"49336207","metadata":{"id":"49336207"},"source":["<H1><c>Creating title Index<H1/>"]},{"cell_type":"code","execution_count":null,"id":"55c8764e","metadata":{"id":"55c8764e","nbgrader":{"grade":false,"grade_id":"cell-index_construction","locked":false,"schema_version":3,"solution":true,"task":false},"outputId":"4ba2ae7d-27fd-402a-b7de-d8067258240e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 12:=================================================>    (114 + 4) / 124]\r"]}],"source":["# word counts map\n","\n","word_counts_title = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","w2df_title = calculate_df(postings_title)\n","w2df_title_dict = w2df_title.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_title).collect()\n","\n","# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix=f'postings_gcp{index_type}'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)\n","    \n","# Create inverted index instance\n","title_index = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","title_index.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","title_index.df = w2df_title_dict\n","# write the global stats out\n","title_index.write_index('.', 'title_index')\n","# upload to gs\n","index_src = \"title_index.pkl\"\n","index_dst = f'gs://{bucket_name}/indexes/{index_src}'\n","!gsutil cp $index_src $index_dst\n","\n","!gsutil ls -lh $index_dst"]},{"cell_type":"code","execution_count":1,"id":"8be350d8","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'postings_title' is not defined","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)","Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mpostings_title\u001B[49m\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m1\u001B[39m))\n","\u001B[0;31mNameError\u001B[0m: name 'postings_title' is not defined"]}],"source":["print(postings_title.take(1))"]},{"cell_type":"code","execution_count":null,"id":"f9e3b269","metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}