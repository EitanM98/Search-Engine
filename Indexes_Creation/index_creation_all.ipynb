{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"a00e032c"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"51cf86c5"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":1,"id":"bf199e6a","metadata":{"id":"bf199e6a","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')\n"]},{"cell_type":"code","execution_count":2,"id":"47900073","metadata":{"id":"47900073","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":3,"id":"980e62a5","metadata":{"id":"980e62a5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'ir_assg3_eithan' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if \"parquet\" in b.name:\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"cac891c2","metadata":{"id":"cac891c2"},"source":["***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"582c3f5e"},"source":["# Building an inverted index"]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"481f2044"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"code","execution_count":7,"id":"e4c523e7","metadata":{"id":"e4c523e7","outputId":"76cfc76a-59a6-4ab7-f993-6b6d58fe7445","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd"]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"0d7e2971"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"code","execution_count":8,"id":"82881fbf","metadata":{"id":"82881fbf"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["6348910"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"code","execution_count":10,"id":"121fe102","metadata":{"id":"121fe102","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":11,"id":"57c101a8","metadata":{"id":"57c101a8","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":12,"id":"c259c402","metadata":{"id":"c259c402"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"353e36d8","metadata":{"id":"353e36d8"},"source":["<H1> Helper function to create Indexes <H1/>"]},{"cell_type":"code","execution_count":22,"id":"2addd40e","metadata":{},"outputs":[],"source":["index_type = \"_body\""]},{"cell_type":"code","execution_count":23,"id":"f3ad8fea","metadata":{"id":"f3ad8fea","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def word_count(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    cnt = Counter([tok for tok in tokens if tok not in all_stopwords])\n","    return [(key, (id, val)) for key,val in cnt.items()]\n","    \n","    \n","def reduce_word_counts(unsorted_pl):\n","    return sorted(unsorted_pl, key= lambda tup: tup[0])\n","\n","\n","def calculate_df(postings):\n","    return postings.map(lambda lm: (lm[0],len(lm[1])))\n","\n","\n","def partition_postings_and_write(postings):\n","    partition_rdd = postings.map(lambda w_pl: (token2bucket_id(w_pl[0]),w_pl)).groupByKey()\n","    return partition_rdd.map(lambda b_w_pl: InvertedIndex.write_a_posting_list(b_w_pl,bucket_name,index_type=index_type))\n","\n","def word_count_binary(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    words_set = set(tok for tok in tokens if tok not in all_stopwords)\n","    return [(word, (id, 1)) for word in words_set]\n","\n","def doc_len_count(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    return (id,len(tokens))\n","\n","# Assignment2 Code: \n","\n","def tokenize(text):\n","  return [token.group() for token in RE_WORD.finditer(text.lower())]\n","\n","def calculate_tf(docs):\n","  word_counts = Counter()\n","  for text in docs:\n","    tokens = tokenize(title)\n","    doc_counts = Counter(tokens)\n","    word_counts.update(doc_counts)\n","  return word_counts"]},{"cell_type":"markdown","id":"5732ad54","metadata":{},"source":["<h1> Dictionaries creation <H1>"]},{"cell_type":"code","execution_count":14,"id":"e51e9e9f","metadata":{},"outputs":[],"source":["doc_len_dict = doc_text_pairs.map(lambda pair: doc_len_count(pair[0],pair[1]))"]},{"cell_type":"code","execution_count":15,"id":"0378e730","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["6348910"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["doc_len_dict.count()"]},{"cell_type":"code","execution_count":18,"id":"1054914b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["DL = doc_len_dict.collectAsMap()"]},{"cell_type":"code","execution_count":21,"id":"03d09f18","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://doc_len_dict.pickle [Content-Type=application/octet-stream]...\n","/ [1 files][ 44.8 MiB/ 44.8 MiB]                                                \n","Operation completed over 1 objects/44.8 MiB.                                     \n"," 44.84 MiB  2023-01-05T15:46:11Z  gs://ir_assg3_eithan/indexes/doc_len_dict.pickle\n","TOTAL: 1 objects, 47014993 bytes (44.84 MiB)\n"]}],"source":["with open('doc_len_dict.pickle', 'wb') as f:\n","    pickle.dump(DL, f)\n","\n","index_src = \"doc_len_dict.pickle\"\n","index_dst = f'gs://{bucket_name}/indexes/{index_src}'\n","!gsutil cp $index_src $index_dst\n","\n","!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"49336207","metadata":{"id":"49336207"},"source":["<H1>Creating Body Small Index<H1/>"]},{"cell_type":"code","execution_count":null,"id":"55c8764e","metadata":{"id":"55c8764e","nbgrader":{"grade":false,"grade_id":"cell-index_construction","locked":false,"schema_version":3,"solution":true,"task":false},"outputId":"4ba2ae7d-27fd-402a-b7de-d8067258240e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 14:====>                                                 (10 + 16) / 124]\r"]}],"source":["# word counts map\n","\n","word_counts_body = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings_body = word_counts_body.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","postings_filtered = postings_body.filter(lambda x: len(x[1])>50)\n","w2df_body = calculate_df(postings_filtered)\n","w2df_body_dict = w2df_body.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered).collect()\n","\n","# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix=f'postings_gcp{index_type}'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)\n","    \n","# Create inverted index instance\n","body_index = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","body_index.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","body_index.df = w2df_body_dict\n","# write the global stats out\n","body_index.write_index('.', 'body_index')\n","# upload to gs\n","index_src = \"body_index.pkl\"\n","index_dst = f'gs://{bucket_name}/indexes/{index_src}'\n","!gsutil cp $index_src $index_dst\n","\n","!gsutil ls -lh $index_dst"]},{"cell_type":"code","execution_count":null,"id":"94a009af","metadata":{"id":"94a009af","outputId":"229d2e15-301c-492c-b0cb-81f08624fd75"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('populous', [(844, 2), (1998, 4), (2377, 1), (3138, 4), (3708, 1), (5407, 15), (8522, 14), (10992, 2), (12521, 2), (14533, 4), (14597, 1), (14598, 2), (15474, 1), (18402, 2), (19058, 1), (19189, 4), (19261, 1), (22309, 2), (22989, 2), (27862, 2), (30057, 3), (31453, 3), (32706, 3), (34361, 1), (36511, 1), (37534, 4), (45470, 4), (47905, 3), (49728, 4), (53446, 7), (53837, 3), (69336, 1), (72214, 1), (124779, 1), (172538, 1), (231623, 4), (302201, 1), (309890, 10), (311130, 1), (314356, 4), (320443, 1), (390875, 7), (453584, 1), (636806, 6), (645042, 8), (1649321, 3), (3799826, 2), (5201333, 1), (6622547, 2), (7564733, 1), (11164587, 1), (15218891, 1), (19159283, 1), (20587357, 1)])]\n"]}],"source":["# from inverted_index_gcp import MultiFileReader\n","# file_reader = MultiFileReader()\n","# print(file_reader.read(body_index.posting_locs[0], 1428))\n","# print(postings_filtered.take(1))"]},{"cell_type":"markdown","id":"8cf14a3d","metadata":{"id":"8cf14a3d"},"source":["<H1> Creating Title Small Index <H1/>"]},{"cell_type":"code","execution_count":null,"id":"dc3d7ce7","metadata":{"id":"dc3d7ce7","outputId":"de194d3b-11e2-4bf1-e51d-f52b29574ec9"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Copying file://title_index.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][324.6 KiB/324.6 KiB]                                                \n","Operation completed over 1 objects/324.6 KiB.                                    \n"]}],"source":["doc_title_pairs = our_parquet.select(\"title\", \"id\").rdd\n","\n","\n","# word counts map\n","word_counts_title = doc_title_pairs.flatMap(lambda x: word_count_binary(x[0], x[1]))\n","postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","#----------------------------------------------On big corpus change back to 50 --------------------------------------\n","# postings_filtered = postings_title.filter(lambda x: len(x[1])>10)\n","w2df_title = calculate_df(postings_title)\n","w2df_title_dict = w2df_title.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered).collect()\n","\n","# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)\n","    \n","# Create inverted index instance\n","title_index = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","title_index.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","title_index.df = w2df_title_dict\n","# write the global stats out\n","title_index.write_index('.', 'title_index')\n","# upload to gs\n","index_src = \"title_index.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp/small_indexes/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"4863d6e1","metadata":{"id":"4863d6e1","outputId":"c93d40a4-2920-4358-b0d1-e2efd4d10a93"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'inc': 11, 'series': 12, 'disorder': 13, 'wine': 33, 'world': 60, 'cup': 55, 'gold': 25, 'java': 40, 'language': 15, 'money': 22, 'netflix': 14, 'marvel': 26, 'united': 12, 'universe': 19, 'rick': 15, 'apple': 21, 'fifa': 47, 'india': 35, 'information': 24, 'retrieval': 28, 'list': 66, 'jordan': 16, 'simpsons': 27, 'winter': 41, 'cannabis': 37, 'cigarette': 36, 'children': 12, 'depression': 26, 'cities': 13, 'coffee': 34, 'weight': 12, 'film': 14, 'states': 11, 'season': 19, 'qualification': 12, '2022': 21, 'morty': 15, 'cinematic': 18}\n"]}],"source":["# print(title_index.df)"]},{"cell_type":"markdown","id":"4020a4e5","metadata":{"id":"4020a4e5"},"source":["<H1> Creating Anchor Text Small Inverted Index <H1/>"]},{"cell_type":"code","execution_count":null,"id":"922267e5","metadata":{"id":"922267e5"},"outputs":[],"source":["doc_anchor_pairs = our_parquet.select(\"anchor_text\", \"id\").rdd\n","\n","def parse_anchor_text(pair):\n","    id = pair[1]\n","    anchor_text = \"\"\n","    for tup in pair[0]:\n","        anchor_text += \" \" + tup[1]\n","    return (anchor_text, id)\n","\n","tmp_pairs = doc_anchor_pairs.map(lambda pair: parse_anchor_text(pair))\n","# print(tmp_pairs.take(1))"]},{"cell_type":"code","execution_count":null,"id":"2b284d5a","metadata":{"id":"2b284d5a","outputId":"3388f32a-d27d-490f-b20c-5c7c3d0ae537"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Copying file://anchor_index.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][  1.0 MiB/  1.0 MiB]                                                \n","Operation completed over 1 objects/1.0 MiB.                                      \n"]}],"source":["\n","    \n","# word counts map\n","word_counts_anchor = tmp_pairs.flatMap(lambda x: word_count_binary(x[0], x[1]))\n","postings_anchor = word_counts_anchor.groupByKey().mapValues(reduce_word_counts)\n","w2df_anchor = calculate_df(postings_anchor)\n","w2df_anchor_dict = w2df_anchor.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered).collect()\n","\n","# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)\n","    \n","# Create inverted index instance\n","anchor_index = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","anchor_index.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","anchor_index.df = w2df_anchor_dict\n","# write the global stats out\n","anchor_index.write_index('.', 'anchor_index')\n","# upload to gs\n","index_src = \"anchor_index.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp/small_indexes/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"2f41e10b","metadata":{"id":"2f41e10b","outputId":"6b72846a-2ed0-45da-f047-131f6160eaea"},"outputs":[{"name":"stdout","output_type":"stream","text":["['camps', 'illegal', 'gerrit', 'planological', 'laan', 'refugee', 'carsharing', 'intercity-express', 'community', 'huguenot', 'count', 'toon', 'keyser', 'eberhard', 'afc', 'vingboons', 'work', 'university', 'netflix', 'mart', 'world', 'uitmarkt', 'schiphol', 'arkefly', 'attractions', 'hendrick', 'new', 'diemen', 'hermitage', 'effect', 'defined', 'school', 'climate', 'vervoerbedrijf', 'sugar', 'zaanstad', 'islam', 'hortus', 'indonesia', 'america', 'uber', 'forbes', 'james', 'revolution', 'molenaar', 'nelis', 'thirty', 'fontainebleau', 'sbs', 'historic', 'social', 'modern', 'lyceum', 'canals', 'bibliotheek', 'toneelgroep', 'inquisition', 'palace', 'bergen-belsen', 'applied', 'magistra', 'raw', 'twelve', 'low', 'asylum', 'austria', 'martinair', 'countries', 'amstelpark', 'vincent', 'green', 'paths', 'scotland', 'rent', 'ensign', 'samuel', 'pearl', 'area', 'east', 'stadium', 'flevopark', 'revolt', 'michel', 'lycÃ©e', 'guest', 'sea', 'concentration', 'reserve', \"king's\", 'tax', 'king', 'museum', 'botanicus', 'tower', 'concertgebouworkest', 'potter', 'raoul', 'religious', 'coldplay', 'comedytrain']\n","61418\n"]}],"source":["# print(list(anchor_index.df.keys())[:100])\n","# print(len(anchor_index.df.items()))\n","# print(len(title_index.df.items()))\n","# print(len(body_index.df.items()))"]},{"cell_type":"markdown","id":"c52dee14","metadata":{"id":"c52dee14","nbgrader":{"grade":false,"grade_id":"cell-2a6d655c112e79c5","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# PageRank"]},{"cell_type":"markdown","id":"0875c6bd","metadata":{"id":"0875c6bd","nbgrader":{"grade":false,"grade_id":"cell-2fee4bc8d83c1e2a","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["**YOUR TASK (10 POINTS):** Compute PageRank for the entire English Wikipedia. Use your implementation for `generate_graph` function from Colab below."]},{"cell_type":"code","execution_count":null,"id":"31a516e2","metadata":{"id":"31a516e2"},"outputs":[],"source":["# Put your `generate_graph` function here\n","def generate_graph(pages):\n","    distinct_rdd =  pages.map(lambda page: (page[0], set(map(lambda id_text: id_text[0], page[1]))))\n","    edges = distinct_rdd.flatMap(lambda src_page: list(map(lambda dst_page: (src_page[0], dst_page),src_page[1])))\n","    vertices = edges.flatMap(lambda edge: [(edge[0],),(edge[1],)]).distinct()\n","    return edges, vertices"]},{"cell_type":"code","execution_count":null,"id":"6bc05ba3","metadata":{"id":"6bc05ba3","nbgrader":{"grade":false,"grade_id":"cell-PageRank","locked":false,"schema_version":3,"solution":true,"task":false},"outputId":"70b4fd4f-c3e4-45b4-8fcd-4542c049b07c"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 326:===============================================>    (184 + 12) / 200]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+------------------+\n","|     id|          pagerank|\n","+-------+------------------+\n","|3434750| 9913.728782160773|\n","|  10568| 5385.349263642039|\n","|  32927| 5282.081575765276|\n","|  30680| 5128.233709604121|\n","|5843419| 4957.567686263867|\n","|  68253| 4769.278265355161|\n","|  31717|  4486.35018054831|\n","|  11867|4146.4146509127695|\n","|  14533| 3996.466440885503|\n","| 645042| 3531.627089803743|\n","|  17867|3246.0983906041415|\n","|5042916|2991.9457391661786|\n","|4689264| 2982.324883041747|\n","|  14532|2934.7468292031717|\n","|  25391| 2903.546223513398|\n","|   5405|2891.4163291546356|\n","|4764461| 2834.366987332661|\n","|  15573| 2783.865118158838|\n","|   9316| 2782.039646413769|\n","|8569916|2775.2861918400163|\n","+-------+------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["t_start = time()\n","pages_links = spark.read.parquet(\"gs://wikidata_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n","pr_time = time() - t_start\n","pr.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}