{"cells":[{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"51cf86c5"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":1,"id":"bf199e6a","metadata":{"id":"bf199e6a","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')\n"]},{"cell_type":"code","execution_count":2,"id":"47900073","metadata":{"id":"47900073","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":3,"id":"980e62a5","metadata":{"id":"980e62a5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'ir_assg3_eithan' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if \"parquet\" in b.name:\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"582c3f5e"},"source":["# Building an inverted index"]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"481f2044"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"code","execution_count":4,"id":"e4c523e7","metadata":{"id":"e4c523e7","outputId":"76cfc76a-59a6-4ab7-f993-6b6d58fe7445","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)"]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"0d7e2971"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"code","execution_count":5,"id":"82881fbf","metadata":{"id":"82881fbf","outputId":"dae44ff3-ef5b-4c81-de7c-fc951b37d1b8"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["6348910"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"code","execution_count":6,"id":"121fe102","metadata":{"id":"121fe102","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":7,"id":"57c101a8","metadata":{"id":"57c101a8","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":8,"id":"c259c402","metadata":{"id":"c259c402"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"353e36d8","metadata":{"id":"353e36d8"},"source":["<H1> Helper function to create Indexes <H1/>"]},{"cell_type":"code","execution_count":9,"id":"U_uqRJ4g2348","metadata":{"id":"U_uqRJ4g2348"},"outputs":[],"source":["# Put your `generate_graph` function here\n","def generate_graph(pages):\n","    distinct_rdd =  pages.map(lambda page: (page[0], set(map(lambda id_text: id_text[0], page[1]))))\n","    edges = distinct_rdd.flatMap(lambda src_page: list(map(lambda dst_page: (src_page[0], dst_page),src_page[1])))\n","    vertices = edges.flatMap(lambda edge: [(edge[0],),(edge[1],)]).distinct()\n","    return edges, vertices"]},{"cell_type":"code","execution_count":null,"id":"mUUHTUbf3Abv","metadata":{"id":"mUUHTUbf3Abv"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 313:=================================================>   (185 + 4) / 200]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+------------------+\n","|     id|          pagerank|\n","+-------+------------------+\n","|3434750|  9913.72878216078|\n","|  10568| 5385.349263642036|\n","|  32927| 5282.081575765279|\n","|  30680| 5128.233709604123|\n","|5843419|  4957.56768626387|\n","|  68253| 4769.278265355163|\n","|  31717| 4486.350180548311|\n","|  11867|  4146.41465091277|\n","|  14533| 3996.466440885504|\n","| 645042| 3531.627089803744|\n","|  17867|3246.0983906041433|\n","|5042916| 2991.945739166179|\n","|4689264| 2982.324883041748|\n","|  14532| 2934.746829203171|\n","|  25391| 2903.546223513399|\n","|   5405| 2891.416329154635|\n","|4764461|2834.3669873326608|\n","|  15573|2783.8651181588393|\n","|   9316|  2782.03964641377|\n","|8569916| 2775.286191840017|\n","+-------+------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["pages_links = parquetFile.select(\"id\", \"anchor_text\").rdd\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr.repartition(1).write.csv(f'gs://{bucket_name}/indexes/pr/page_rank_dict.csv', compression=\"gzip\")\n","pr.show()"]},{"cell_type":"code","execution_count":null,"id":"50f374b1","metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}