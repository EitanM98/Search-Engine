{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e402a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: nltk==3.7 in /opt/conda/miniconda3/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk==3.7) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk==3.7) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk==3.7) (4.64.1)\n",
      "Requirement already satisfied: click in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk==3.7) (8.1.3)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: Flask in /opt/conda/miniconda3/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Flask) (2.1.2)\n",
      "Requirement already satisfied: click>=8.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Flask) (8.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Flask) (3.0.3)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Flask) (2.2.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Flask) (5.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=3.6.0->Flask) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Jinja2>=3.0->Flask) (2.1.1)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes\n",
    "!pip install nltk==3.7\n",
    "!pip install -U Flask\n",
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import hashlib\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480dd9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = storage.Client()\n",
    "# bucket_name = 'ir_assg3_eithan'\n",
    "\n",
    "# bucket = client.bucket(bucket_name)\n",
    "\n",
    "# # Get a list of all the blobs in the folder\n",
    "# blobs = bucket.list_blobs(prefix='indexes/')\n",
    "\n",
    "# # Iterate over the list of blobs and download each one to a local file\n",
    "# for blob in blobs:\n",
    "#     if 'doc_norm_dict.pickle' in blob.name or 'doc_len_dict.pickle' in blob.name:\n",
    "#         local_file_name = blob.name[blob.name.rfind(\"/\")+1:]\n",
    "#         blob.download_to_filename(local_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5555d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/conda/miniconda3/lib/python3.8/site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from fasttext) (2.10.3)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from fasttext) (59.8.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.8/site-packages (from fasttext) (1.24.1)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: gensim in /opt/conda/miniconda3/lib/python3.8/site-packages (4.3.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from gensim) (1.24.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/miniconda3/lib/python3.8/site-packages (from FuzzyTM>=0.4.0->gensim) (1.2.5)\n",
      "Requirement already satisfied: pyfume in /opt/conda/miniconda3/lib/python3.8/site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.6)\n",
      "Requirement already satisfied: simpful in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.9.0)\n",
      "Requirement already satisfied: fst-pso in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in /opt/conda/miniconda3/lib/python3.8/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in /opt/conda/miniconda3/lib/python3.8/site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.25.11)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.10)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import collections\n",
    "import csv\n",
    "import gzip\n",
    "import math\n",
    "import json\n",
    "\n",
    "!pip install fasttext\n",
    "import fasttext\n",
    "import requests\n",
    "\n",
    "!pip install gensim\n",
    "# from gensim.models.wrappers import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf43d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dataproc\n"
     ]
    }
   ],
   "source": [
    "%cd /home/dataproc\n",
    "from inverted_index_gcp import InvertedIndex\n",
    "# from inverted_index_gcp import *\n",
    "# import inverted_index_gcp as iig\n",
    "from inverted_index_gcp import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1aec54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import importlib\n",
    "# importlib.reload(iig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe924cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_anchor = \"/home/dataproc/postings_gcp_anchor\"\n",
    "path_body = \"/home/dataproc/postings_gcp_body\"\n",
    "path_title = \"/home/dataproc/postings_gcp_title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf4405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read indexes\n",
    "body_index = InvertedIndex.read_index('.',\"body_index\")\n",
    "anchor_index = InvertedIndex.read_index('.',\"anchor_index\")\n",
    "title_index = InvertedIndex.read_index('.',\"title_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2edebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_index.bin_path = path_body\n",
    "anchor_index.bin_path = path_anchor\n",
    "title_index.bin_path = path_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ba0e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc_id_title_dict.pickle','rb') as f:\n",
    "    doc_title_dict = pickle.load(f)\n",
    "    \n",
    "with open('doc_len_dict.pickle','rb') as f:\n",
    "    doc_len_dict = pickle.load(f)\n",
    "    \n",
    "with open('page_views_dict.pkl','rb') as f:\n",
    "    page_views_dict = pickle.load(f)\n",
    "\n",
    "with open('page_rank_dict.pickle','rb') as f:\n",
    "    page_rank_dict = pickle.load(f)\n",
    "\n",
    "with open('doc_norm_dict.pickle','rb') as f:\n",
    "    doc_norm_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c63860",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('queries control group.json', 'rt') as f:\n",
    "    queries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b092dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "def read_posting_list(inverted, w, index_type=\"\"):\n",
    "    with closing(MultiFileReader()) as reader:\n",
    "        locs = inverted.posting_locs[w]\n",
    "        b = reader.read(locs, inverted.df[w] * TUPLE_SIZE,index_type)\n",
    "        posting_list = []\n",
    "        for i in range(inverted.df[w]):\n",
    "            doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
    "            tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
    "            posting_list.append((doc_id, tf))\n",
    "        return posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e9281ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading posting list example:\n",
    "# lst = read_posting_list(body_index,\"texas\", path_body)\n",
    "# print(len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ab5f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    tokens = set(tok for tok in tokens if tok not in all_stopwords)\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90c494f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_title():\n",
    "    ''' Returns ALL (not just top 100) search results that contain A QUERY WORD \n",
    "        IN THE TITLE of articles, ordered in descending order of the NUMBER OF \n",
    "        QUERY WORDS that appear in the title. For example, a document with a \n",
    "        title that matches two of the query words will be ranked before a \n",
    "        document with a title that matches only one query term. \n",
    "\n",
    "        Test this by navigating to the a URL like:\n",
    "         http://YOUR_SERVER_DOMAIN/search_title?query=hello+world\n",
    "        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n",
    "        if you're using ngrok on Colab or your external IP on GCP.\n",
    "    Returns:\n",
    "    --------\n",
    "        list of ALL (not just top 100) search results, ordered from best to \n",
    "        worst where each element is a tuple (wiki_id, title).\n",
    "    '''\n",
    "\n",
    "    # query=[\"hello world\"] - > [(wiki_id, title)] sorted decreasing\n",
    "    # in a way that the title that contains most words of the query\n",
    "\n",
    "    res = []\n",
    "#     query = request.args.get('hello world', '')\n",
    "    query = \"LinkedIn\"\n",
    "#     query = \"hello\"\n",
    "    if len(query) == 0:\n",
    "        return jsonify(res)\n",
    "    # BEGIN SOLUTION\n",
    "\n",
    "    counter = collections.Counter()\n",
    "    for token in tokenize(query):\n",
    "        for docId_tf in title_index.read_posting_list(token):\n",
    "            doc_id = docId_tf[0]\n",
    "            title = doc_title_dict[doc_id]\n",
    "            counter[(doc_id, title)] += 1\n",
    "\n",
    "    \n",
    "#     res = [map(lambda tup:tup[0], counter.most_common())] \n",
    "    res = list(map(lambda tup:tup[0], counter.most_common()))\n",
    "    # END SOLUTION\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b800ad24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(970755, 'LinkedIn'), (27769500, 'LinkedIn Pulse'), (36070366, '2012 LinkedIn hack'), (41726116, 'LinkedIn Learning'), (50191962, 'Timeline of LinkedIn'), (57147095, 'LinkedIn Top Companies'), (62976368, 'HiQ Labs v. LinkedIn')]\n"
     ]
    }
   ],
   "source": [
    "print(search_title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e881dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_anchor():\n",
    "    # query=[\"hello world\"] - > [(wiki_id, title)] sorted decreasing\n",
    "    # in a way that the title that contains most words of the query\n",
    "\n",
    "    res = []\n",
    "#     query = request.args.get('hello world', '')\n",
    "    query = \"LinkedIn\"\n",
    "#     query = \"hello\"\n",
    "    if len(query) == 0:\n",
    "        return jsonify(res)\n",
    "    # BEGIN SOLUTION\n",
    "\n",
    "    counter_anchor = collections.Counter()\n",
    "    for token in tokenize(query):\n",
    "        for docId_tf in anchor_index.read_posting_list(token):\n",
    "            doc_id = docId_tf[0]\n",
    "            title = doc_title_dict[doc_id]\n",
    "            counter_anchor[(doc_id, title)] += 1\n",
    "\n",
    "    \n",
    "#     res = [map(lambda tup:tup[0], counter.most_common())] \n",
    "    res = list(map(lambda tup:tup[0], counter_anchor.most_common()))\n",
    "    # END SOLUTION\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a58d58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(search_anchor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95b06982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pageview():\n",
    "    ''' Returns the number of page views that each of the provide wiki articles\n",
    "        had in August 2021.\n",
    "\n",
    "        Test this by issuing a POST request to a URL like:\n",
    "          http://YOUR_SERVER_DOMAIN/get_pageview\n",
    "        with a json payload of the list of article ids. In python do:\n",
    "          import requests\n",
    "          requests.post('http://YOUR_SERVER_DOMAIN/get_pageview', json=[1,5,8])\n",
    "        As before YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n",
    "        if you're using ngrok on Colab or your external IP on GCP.\n",
    "    Returns:\n",
    "    --------\n",
    "        list of ints:\n",
    "          list of page view numbers from August 2021 that correrspond to the \n",
    "          provided list article IDs.\n",
    "    '''\n",
    "\n",
    "    res = []\n",
    "#     wiki_ids = request.get_json()\n",
    "    wiki_ids = [848, 21105323, 23536538, 23797577]\n",
    "    if len(wiki_ids) == 0:\n",
    "        return res\n",
    "    # BEGIN SOLUTION\n",
    "    \n",
    "    res = [page_views_dict.get(doc_id, -1) for doc_id in wiki_ids]\n",
    "\n",
    "    # END SOLUTION\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be6bd398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84370, 1042, 30483, 38197]\n"
     ]
    }
   ],
   "source": [
    "print(get_pageview())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "007f8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagerank():\n",
    "    ''' Returns PageRank values for a list of provided wiki article IDs. \n",
    "\n",
    "        Test this by issuing a POST request to a URL like:\n",
    "          http://YOUR_SERVER_DOMAIN/get_pagerank\n",
    "        with a json payload of the list of article ids. In python do:\n",
    "          import requests\n",
    "          requests.post('http://YOUR_SERVER_DOMAIN/get_pagerank', json=[1,5,8])\n",
    "        As before YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n",
    "        if you're using ngrok on Colab or your external IP on GCP.\n",
    "    Returns:\n",
    "    --------\n",
    "        list of floats:\n",
    "          list of PageRank scores that correrspond to the provided article IDs.\n",
    "    '''\n",
    "    res = []\n",
    "#     wiki_ids = request.get_json()\n",
    "    wiki_ids = [848, 3434750, 32927]\n",
    "    if len(wiki_ids) == 0:\n",
    "        return jsonify(res)\n",
    "    # BEGIN SOLUTION\n",
    "\n",
    "    res = [page_rank_dict.get(doc_id, -1) for doc_id in wiki_ids]\n",
    "    # END SOLUTION\n",
    "#     return jsonify(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bad9d36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50.7020900020747, 9913.728782160773, 5282.081575765278]\n"
     ]
    }
   ],
   "source": [
    "print(get_pagerank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44502b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "N = 6348910\n",
    "\n",
    "def normalize(tokens):\n",
    "    counter = Counter(tokens)\n",
    "    norm = 0\n",
    "    for value in counter.values():\n",
    "        norm += value*value\n",
    "    if norm == 0 :\n",
    "        return 0\n",
    "    return 1/(math.sqrt(norm))\n",
    "\n",
    "\n",
    "def tf_idf_calc (token, doc_id, tf):\n",
    "    tf =  tf/doc_len_dict[doc_id]\n",
    "#     idf = math.log(N/app.body_index.df[token], 2)\n",
    "    idf = math.log(N/body_index.df[token], 2)\n",
    "    return tf*idf\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    tokens = [tok for tok in tokens if tok not in all_stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ade5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_body():\n",
    "    ''' Returns up to a 100 search results for the query using TFIDF AND COSINE\n",
    "        SIMILARITY OF THE BODY OF ARTICLES ONLY. DO NOT use stemming. DO USE the \n",
    "        staff-provided tokenizer from Assignment 3 (GCP part) to do the \n",
    "        tokenization and remove stopwords. \n",
    "\n",
    "        To issue a query navigate to a URL like:\n",
    "         http://YOUR_SERVER_DOMAIN/search_body?query=hello+world\n",
    "        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n",
    "        if you're using ngrok on Colab or your external IP on GCP.\n",
    "    Returns:\n",
    "    --------\n",
    "        list of up to 100 search results, ordered from best to worst where each \n",
    "        element is a tuple (wiki_id, title).\n",
    "    '''\n",
    "    res = []\n",
    "#     query = request.args.get('query', '')\n",
    "    query = 'linkedIn'\n",
    "    if len(query) == 0:\n",
    "        return jsonify(res)\n",
    "    # BEGIN SOLUTION\n",
    "\n",
    "    similarity_dict = {}\n",
    "    tokens = tokenize(query)\n",
    "    for token in tokens:\n",
    "        for doc_tf in body_index.read_posting_list(token):\n",
    "            if doc_tf[0] not in similarity_dict:\n",
    "                similarity_dict[doc_tf[0]] = 0\n",
    "            similarity_dict[doc_tf[0]] += tf_idf_calc(token, doc_tf[0], doc_tf[1])\n",
    "\n",
    "    for doc in similarity_dict.keys():\n",
    "        similarity_dict[doc] = similarity_dict[doc]*normalize(tokens)*doc_norm_dict[doc]\n",
    "        \n",
    "    top100 = list(sorted(similarity_dict.items(), key=lambda item: item[1], reverse=True)[:100])\n",
    "    \n",
    "    for pair in top100:\n",
    "        res.append((pair[0], doc_title_dict[pair[0]]))\n",
    "\n",
    "    # END SOLUTION\n",
    "#     return jsonify(res)\n",
    "#     return res\n",
    "    return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b72f6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(42531713, 'Jonathan Morgan (tennis)'), (68308072, 'Alejandra Pérez Lecaros'), (22291773, 'Daniel Roth'), (47626712, 'Yoel Bouza'), (560176, 'Craig McClanahan'), (28871639, 'Salvatore Falco'), (67611948, 'Jan Laugesen'), (35902457, 'Shree Kitini (10+2) Higher Secondary School'), (59144735, 'Apache Helix'), (52157350, 'Tamara Shukakidze'), (44295156, 'Stacy Christakakis'), (68338141, 'Elisa Giustinianovich'), (57095061, 'Connectifier'), (37784208, 'Steven Takiff'), (22674192, 'Michael Theodoulou'), (64418727, 'Knowledge Graph (disambiguation)'), (36703543, 'Savio Barnes'), (42738347, 'Mohabir Anil Nandlall'), (50070773, 'Timothy Jurka'), (35934951, 'SocialIQ'), (9516775, 'Charissa Tansomboon'), (67910832, 'Tomer Eiges'), (49277625, 'Ahillen Beadle'), (12249721, 'Mark Hoplamazian'), (63641225, 'Ryan Roslansky'), (49106425, 'Peter Skomoroch'), (29720689, 'Stanley B. Lippman'), (43652569, 'Brandi Hitt'), (38613414, 'Bright.com'), (60996273, 'Ibrahim Badr'), (45248740, 'RenewableUK Cymru'), (41726116, 'LinkedIn Learning'), (17420853, 'Jordan Mendelson'), (53103701, 'Dylan Higgins'), (31987736, 'Zion Matriculation Higher Secondary School'), (55194684, 'Monica Rogati'), (25394492, 'Lydia Nsekera'), (42188864, 'Trulioo'), (51445015, 'The Challenger Sale'), (65086911, 'Chrisi Karvonides-Dushenko'), (5716998, 'Ámbito Financiero'), (68236924, 'Heston Airlines'), (53251023, 'Hotel Interlaken'), (23929191, 'Jesper Pilegaard'), (27912768, 'Robert Beeson'), (42009081, 'Brigade Group'), (62585938, 'Emma Powell'), (15191296, 'Brook Barrington'), (25384630, 'Bernie Mullin'), (44611670, 'Marcos Gusmão'), (63183043, 'Isabelle Rosabrunetto'), (35549457, 'Jeff Weiner'), (4736533, 'Alec Sabin'), (5353827, 'Gornitzky & Co.'), (62786086, 'Jason Trinder'), (37396421, 'Elixio'), (64404276, 'Cassidy Benintente'), (42672986, 'Apache Samza'), (43141026, 'Krzysztof Pawlikowski'), (31839258, 'Blog comment hosting service'), (63379003, 'Ganesh Suntharalingam'), (57636793, 'Kevin Scott (computer scientist)'), (16370324, 'Ambu (company)'), (53321154, 'ICWATCH'), (58659625, 'Techapella'), (8410116, 'Gil Penchina'), (51137597, 'Environment Coastal & Offshore (ECO)'), (66191060, 'Yin Paradies'), (40937804, 'Siemens Mexico'), (37756284, 'Trade Association Forum'), (42534732, 'David De La Peralle'), (59919470, 'Mobi (company)'), (63082618, 'Tom Hurd (civil servant)'), (67613502, 'Shukrani Manya'), (21795752, 'AMIN Worldwide'), (1896160, 'Andy Wellings'), (14595193, 'Yahoo! Kickstart'), (46360973, 'Shinobu Toyoda'), (44189655, 'Graffiti USA'), (68191472, 'Charley McMillan-Lopez'), (32379280, 'Gary Wheaton'), (44760759, 'Anphabe.com'), (36070366, '2012 LinkedIn hack'), (62992396, 'Penny Herscher'), (24284645, 'Energy Delta Institute'), (11086284, 'Socialight'), (47916188, 'VLAB'), (41995847, 'Clem Captein'), (40494989, 'The Coordinating Secretariat for Science, Technology and Innovation (COSTI)'), (43452955, 'Wars and Treasure'), (7505815, 'Stephen Douglas (journalist)'), (62872086, 'Kate Croser'), (40853416, 'Six Degrees patent'), (28666660, 'Kevin J. Collins'), (17403692, 'DenTek'), (1633346, 'Fredric Lebow'), (46758004, 'Shree Pritam'), (48171826, 'Deepa Devasena'), (67234502, 'Jason Gilmore'), (62930727, 'Dolly Parton challenge')]\n"
     ]
    }
   ],
   "source": [
    "print(search_body())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d836ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-11 11:41:58--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10356881291 (9.6G) [application/zip]\n",
      "Saving to: ‘wiki.en.zip’\n",
      "\n",
      "wiki.en.zip         100%[===================>]   9.65G  38.2MB/s    in 4m 10s  \n",
      "\n",
      "2023-01-11 11:46:08 (39.5 MB/s) - ‘wiki.en.zip’ saved [10356881291/10356881291]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.bin\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd8b919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dataproc\n"
     ]
    }
   ],
   "source": [
    "%cd /home/dataproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd54cda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674.0488855666746\n",
      "1.0000000000249587\n"
     ]
    }
   ],
   "source": [
    "page_views_avg = sum(page_views_dict.values())/len(page_views_dict.values())\n",
    "print (page_views_avg)\n",
    "page_rank_avg = sum(page_rank_dict.values())/len(page_rank_dict.values())\n",
    "print(page_rank_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d9d6a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_update(token, doc_id, tf):\n",
    "    k1 = 1.5\n",
    "    B = 0.75\n",
    "    N = 6348910\n",
    "    avgl = 319.5242353411845\n",
    "    idf = math.log(((N - body_index.df[token] + 0.5)/(body_index.df[token]+0.5)) + 1, math.e)\n",
    "    score = (idf * ((tf * (k1 + 1)) / (tf + k1 * (1 - B + B * (doc_len_dict[doc_id] / avgl)))))\n",
    "    return score\n",
    "\n",
    "def search(q, weights):\n",
    "    ''' Returns up to a 100 of your best search results for the query. This is \n",
    "        the place to put forward your best search engine, and you are free to\n",
    "        implement the retrieval whoever you'd like within the bound of the \n",
    "        project requirements (efficiency, quality, etc.). That means it is up to\n",
    "        you to decide on whether to use stemming, remove stopwords, use \n",
    "        PageRank, query expansion, etc.\n",
    "\n",
    "        To issue a query navigate to a URL like:\n",
    "         http://YOUR_SERVER_DOMAIN/search?query=hello+world\n",
    "        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n",
    "        if you're using ngrok on Colab or your external IP on GCP.\n",
    "    Returns:\n",
    "    --------\n",
    "        list of up to 100 search results, ordered from best to worst where each \n",
    "        element is a tuple (wiki_id, title).\n",
    "    '''\n",
    "    res = []\n",
    "#     query = request.args.get('query', '')\n",
    "#     query = 'best marvel movie'\n",
    "    query = q\n",
    "    if len(query) == 0:\n",
    "        return jsonify(res)\n",
    "    # BEGIN SOLUTION\n",
    "\n",
    "    similarity_dict = {}\n",
    "\n",
    "    tokens = tokenize(query)\n",
    "    # word2vec\n",
    "    # Constants\n",
    "    BODY_WEIGHT, ANCHOR_WEIGHT, TITLE_WEIGHT, PAGE_RANK_WEIGHT, PAGE_VIEW_WEIGHT = weights\n",
    "#     print(BODY_WEIGHT)\n",
    "#     print(ANCHOR_WEIGHT)\n",
    "#     print(TITLE_WEIGHT)\n",
    "\n",
    "\n",
    "#     BODY_WEIGHT = 0.5\n",
    "#     ANCHOR_WEIGHT = 2\n",
    "#     TITLE_WEIGHT = 3\n",
    "#     PAGE_RANK_WEIGHT = 0.001\n",
    "#     PAGE_VIEW_WEIGHT = 0.35\n",
    "    token_doc_titles_occurrences = Counter()\n",
    "    token_doc_anchor_occurrences = Counter()\n",
    "\n",
    "    for token in tokens:\n",
    "        # Searching the term in anchor index\n",
    "        for doc_tf in anchor_index.read_posting_list(token):\n",
    "            token_doc_anchor_occurrences[doc_tf[0]] = 1 + token_doc_anchor_occurrences.get(doc_tf[0], 0)\n",
    "\n",
    "        # Searching the term in title index\n",
    "        for doc_tf in title_index.read_posting_list(token):\n",
    "            token_doc_titles_occurrences[doc_tf[0]] = 1 + token_doc_titles_occurrences.get(doc_tf[0], 0)\n",
    "\n",
    "        # Searching the term in body index\n",
    "        for doc_tf in body_index.read_posting_list(token):\n",
    "            if doc_tf[0] not in similarity_dict.keys():\n",
    "                similarity_dict[doc_tf[0]] = 0\n",
    "\n",
    "            a = token_doc_anchor_occurrences.get(doc_tf[0], 0) * ANCHOR_WEIGHT\n",
    "            b = token_doc_titles_occurrences.get(doc_tf[0], 0) * TITLE_WEIGHT\n",
    "            c = bm25_update(token, doc_tf[0], doc_tf[1]) * BODY_WEIGHT\n",
    "            d = math.log((page_rank_dict.get(doc_tf[0], 0.00001)/page_rank_avg),10) * PAGE_RANK_WEIGHT\n",
    "            e = math.log((page_views_dict.get(doc_tf[0], 0.00001)/page_views_avg), 10) * PAGE_VIEW_WEIGHT\n",
    "            \n",
    "#             if doc_tf[0] in [21148, 5043544, 32070]:\n",
    "#                 print('a: '+ str(a/ANCHOR_WEIGHT))\n",
    "#                 print('b: '+ str(b/TITLE_WEIGHT))\n",
    "#                 print('c: '+ str(c/BODY_WEIGHT))\n",
    "#                 print('d: '+ str(d/PAGE_RANK_WEIGHT))\n",
    "            update =  a + b + c + d + e\n",
    "#             update =  a + b + c + d \n",
    "#             update =  d + b + c  \n",
    "            similarity_dict[doc_tf[0]] += update\n",
    "\n",
    "\n",
    "    top100 = list(sorted(similarity_dict.items(), key=lambda item: item[1], reverse=True)[:100])\n",
    "\n",
    "    for pair in top100:\n",
    "        res.append((pair[0], doc_title_dict[pair[0]]))\n",
    "\n",
    "    # END SOLUTION\n",
    "#     return jsonify(res)\n",
    "#     return top100\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2f9db432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(268524, 'Ship of Theseus'), (855210, 'Lead ship'), (164340, 'Ship prefix'), (60925, 'Ship commissioning'), (4115690, 'Khufu ship'), (4218865, 'Oasis-class cruise ship'), (35013518, 'The captain goes down with the ship'), (291618, \"Her Majesty's Ship\"), (187604, 'Vasa (ship)'), (1138566, 'Full-rigged ship'), (1061074, 'Oseberg Ship'), (102848, 'White Ship'), (1354688, 'Ghost Ship (2002 film)'), (17020474, 'List of ship types'), (35964309, 'List of ship directions'), (1128147, 'Turtle ship'), (3561821, 'Sister ship'), (93099, 'Ship of the line'), (146351, 'Hospital ship'), (875717, 'Merchant ship'), (723505, 'Gokstad ship'), (786056, 'Cargo ship'), (198016, 'Museum ship'), (407592, 'Hulk (ship type)'), (1037955, 'Ship breaking'), (633917, 'Ghost ship'), (3249592, 'Auxiliary ship'), (198201, 'Liberty ship'), (31186721, 'Quantum-class cruise ship'), (2724407, 'Rotor ship'), (17160742, 'Displacement (ship)'), (2293646, 'Endurance (1912 ship)'), (198199, 'Victory ship'), (491457, 'United States Ship'), (34529535, 'Chittagong Ship Breaking Yard'), (304374, 'Tall ship'), (85620, 'Generation ship'), (3291706, 'Motor ship'), (262084, 'Container ship'), (67782, 'Sailing ship'), (60930, 'Ceremonial ship launching'), (12390676, 'Barracks ship'), (4109253, 'List of auxiliary ship classes in service'), (3228043, 'Viking Ship Museum (Oslo)'), (5456327, 'Ship motions'), (68202898, 'Alang Ship Breaking Yard'), (62010, 'Arsenal ship'), (14318464, 'Amphibious assault ship'), (5833905, 'Tracking ship'), (1359031, 'Training ship'), (1958969, 'Stealth ship'), (247778, 'List of ship names of the Royal Navy'), (27008, 'Ship'), (2678619, 'Stabilizer (ship)'), (1748812, 'Stone ship'), (30969057, 'Borobudur ship'), (24880329, 'Bounty (1960 ship)'), (30967040, 'Ship management'), (33915603, 'Desire (ship)'), (2598788, 'Spanish amphibious assault ship Juan Carlos I'), (641435, 'Dock landing ship'), (2971610, 'Ship Island (Mississippi)'), (3546543, 'Diamond Princess (ship)'), (769395, 'International Ship and Port Facility Security Code'), (2980893, 'Amphibious command ship'), (2480342, 'Ship of State'), (58552731, 'Jacklyn (ship)'), (5987319, 'Ghost Ship (disambiguation)'), (5155011, 'Freedom-class cruise ship'), (61414467, 'Bhoot – Part One: The Haunted Ship'), (8007336, 'Depot ship'), (9865961, 'Type 908 replenishment ship'), (14396096, 'Ship grounding'), (370666, 'Passenger ship'), (64651810, 'HMM Algeciras-class container ship'), (4269876, 'Ship chandler'), (7504439, 'Sailing Ship Columbia'), (20891915, 'Ship mill'), (4264157, 'Burning Ship fractal'), (866066, 'HMS Surprise (replica ship)'), (5774572, 'Ship resistance and propulsion'), (2521578, 'Spy ship'), (5197187, 'Ship (disambiguation)'), (5231762, 'Poseidon (fictional ship)'), (9851858, 'Ship Simulator (video game)'), (10311204, 'Gadani Ship Breaking Yard'), (97765, 'Ship of fools'), (328107, 'Fire ship'), (27986687, 'Chinese hospital ship Daishan Dao'), (68193537, 'Aliağa Ship Breaking Yard'), (18645664, 'Type C4-class ship'), (5526395, \"Don't Give Up the Ship\"), (37376267, 'Ghost Ship of Northumberland Strait'), (4859904, 'Chinese treasure ship'), (43391187, 'Abandon Ship'), (685024, 'Freedom Ship'), (411337, 'Lewis and Clark-class dry cargo ship'), (60577080, 'List of minelayer ship classes'), (3090195, 'Houston Ship Channel'), (19799205, 'Type 0891A training ship')]\n"
     ]
    }
   ],
   "source": [
    "q = \"ship\"\n",
    "print(search(q, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0c5a76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = [0.75133306, 0.08561443, 0.74563351, 0.97631322, 0.54191462]\n",
    "# print(search('Rick and Morty' ,weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e13e6268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1501208493870428\n"
     ]
    }
   ],
   "source": [
    "print(page_rank_dict[68380237])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "68a1bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(queries['Winter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f28024b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(true_list, predicted_list, k=40):\n",
    "    true_set = frozenset(true_list)\n",
    "    predicted_list = predicted_list[:k]\n",
    "    precisions = []\n",
    "    for i,doc_id in enumerate(predicted_list): \n",
    "        if doc_id in true_set:\n",
    "            prec = (len(precisions)+1) / (i+1)            \n",
    "            precisions.append(prec)\n",
    "#     print(\"recall: \" + len(precisions)+\" out of 40\")\n",
    "    if len(precisions) == 0:\n",
    "        return 0.0\n",
    "    return round(sum(precisions)/len(precisions),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1c8e4dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1511596, 52709838, 1673945, 17349106, 8521120, 3060382, 16615604, 8438818, 8351234, 979072, 64928991, 36439749, 1817908, 1971153, 30276826, 4538366, 1298502, 22933429, 6511088, 38950, 19431459, 1221144, 19938267, 1843684, 34069, 6201653, 33672235, 3548574, 1372169, 65601132, 38416091, 1088531, 2020857, 316711, 34061, 9825536, 22190045, 43343961, 1632099, 962053, 961505]\n"
     ]
    }
   ],
   "source": [
    "print(queries[\"Winter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "75a7689c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652\n"
     ]
    }
   ],
   "source": [
    "def get_ap(q_title, weights_list):\n",
    "    res = search(q_title ,weights)\n",
    "    predicted, _ = zip(*res)\n",
    "    actual = queries[q_title]\n",
    "    ap = average_precision(actual, predicted)\n",
    "    return ap\n",
    "\n",
    "q_title = \"World Cup 2022\"\n",
    "# weights = [0.75133306, 0.08561443, 0.74563351, 0.97631322, 0.54191462]\n",
    "# Winter query weights: \n",
    "weights = [0.55754871, 1.25785412, 0.54434859, 0.09795087, 0.38754463]\n",
    "print(get_ap(q_title, weights))\n",
    "# actual = [54046846, 49170369, 55339286, 62417830, 54251265, 41283158, 65819511, 49029294, 67520032, 49134382, 42311608, 63656330, 52261594, 41185040, 43794572, 64413225, 49131135, 49127974, 43794574, 63656361, 49128142, 41699729, 51759111, 47762921, 55708102, 63656365, 55339303, 57390230, 67830379, 61805032, 26091326, 54802759]\n",
    "# weights = [0.75133306, 0.08561443, 0.74563351, 0.97631322, 0.54191462]\n",
    "# res = search('Rick and Morty' ,weights)\n",
    "# predicted, _ = zip(*res)\n",
    "# ap = average_precision(actual, predicted)\n",
    "# print(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "36285004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def queries_ap(weights, q_num=1):\n",
    "#     queries = list(queries.items())[:q_num]\n",
    "#     qs_res = []\n",
    "\n",
    "#     for q, true_wids in queries:\n",
    "#         duration, ap = None, None\n",
    "#         t_start = time()\n",
    "#         pred_wids = search(q, weights)\n",
    "#         duration = time() - t_start\n",
    "#         ap = average_precision(true_wids, pred_wids)\n",
    "#         qs_res.append((q, duration, ap, weights))\n",
    "#     return qs_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6f4f45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queries_ap(weights, query):\n",
    "#     queries = list(queries.items())[:q_num]\n",
    "#     qs_res = []\n",
    "    queries = [query]\n",
    "    for q, true_wids in queries:\n",
    "        ap = None\n",
    "#         -----------------------------------------------------\n",
    "        res = search(q, weights)\n",
    "        pred_wids, _ = zip(*res)\n",
    "        ap = average_precision(true_wids, pred_wids)\n",
    "#         qs_res.append((q, duration, ap, weights))\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "de3523d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Winter', [1511596, 52709838, 1673945, 17349106, 8521120, 3060382, 16615604, 8438818, 8351234, 979072, 64928991, 36439749, 1817908, 1971153, 30276826, 4538366, 1298502, 22933429, 6511088, 38950, 19431459, 1221144, 19938267, 1843684, 34069, 6201653, 33672235, 3548574, 1372169, 65601132, 38416091, 1088531, 2020857, 316711, 34061, 9825536, 22190045, 43343961, 1632099, 962053, 961505])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# query_to_run = (q_title, queries[q_title])\n",
    "# print(query_to_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5cf7aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def print_optimized_weights(q_title):\n",
    "    query_to_run = (q_title, queries[q_title])\n",
    "#     print(query_to_run)\n",
    "    # Define the function to be maximized\n",
    "    def y(x):\n",
    "        x1, x2, x3, x4, x5 = x\n",
    "        weights = x.tolist()\n",
    "        res =  queries_ap(weights,query_to_run)\n",
    "        return -1 * res\n",
    "\n",
    "    # Define the initial point\n",
    "    x0 = np.random.uniform(0, 1, 5)\n",
    "\n",
    "    # Number of iterations\n",
    "    num_iterations = 10\n",
    "\n",
    "    # list to hold the best points\n",
    "    best_points = []\n",
    "\n",
    "    # Perform the optimization num_iterations times\n",
    "    for i in range(num_iterations):\n",
    "        # Optimize the function\n",
    "        bounds = [(0, None)]*5\n",
    "\n",
    "        result = minimize(y, x0, bounds=bounds, method='L-BFGS-B')\n",
    "        # add the point to the best points list\n",
    "        best_points.append((-result.fun, result.x))\n",
    "        x0 = result.x\n",
    "        # Randomize x0 a little bit to avoid getting stuck in a local minimum\n",
    "        x0 += np.random.normal(scale=0.1, size=5)\n",
    "#         print(x0)\n",
    "    # Sort the list of best points and take the top 10\n",
    "    best_points.sort(key= lambda y_x: y_x[0] ,reverse=True)\n",
    "    top_10 = best_points[:10]\n",
    "\n",
    "    # Print the 10 best points\n",
    "    print(\"anchor, title, body, page_rank, page_views\")\n",
    "    for i, point in enumerate(top_10):\n",
    "        print(\"{}. y = {}, x = {}\".format(i+1, point[0], point[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0686807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72003659 0.62859251 0.41320103 0.64153128 0.45821703]\n",
      "[0.70467526 0.67284511 0.50217344 0.63171468 0.40092812]\n",
      "[0.73573968 0.73870666 0.52359982 0.61458126 0.48539368]\n",
      "[0.53134471 0.84669479 0.49451533 0.46530829 0.5732673 ]\n",
      "[0.6462671  0.75457893 0.61579258 0.64595072 0.55396991]\n",
      "[0.69688303 0.6558302  0.57247377 0.58757528 0.73187791]\n",
      "[0.70745614 0.70524924 0.64024859 0.53248478 0.7185971 ]\n",
      "[0.77795504 0.66104408 0.76544221 0.65682795 0.74708016]\n",
      "[0.66565863 0.71939317 0.85524436 0.6167213  0.74657466]\n",
      "[0.5662838  0.67381974 0.78139254 0.5649084  0.72723188]\n",
      "anchor, title, body, page_rank, page_views\n",
      "1. y = 0.818, x = [0.66565863 0.71939317 0.85524436 0.6167213  0.74657466]\n",
      "2. y = 0.816, x = [0.77795504 0.66104408 0.76544221 0.65682795 0.74708016]\n",
      "3. y = 0.813, x = [0.5662838  0.67381974 0.78139254 0.5649084  0.72723188]\n",
      "4. y = 0.81, x = [0.70745614 0.70524924 0.64024859 0.53248478 0.7185971 ]\n",
      "5. y = 0.807, x = [0.6462671  0.75457893 0.61579258 0.64595072 0.55396991]\n",
      "6. y = 0.802, x = [0.69688303 0.6558302  0.57247377 0.58757528 0.73187791]\n",
      "7. y = 0.794, x = [0.72003659 0.62859251 0.41320103 0.64153128 0.45821703]\n",
      "8. y = 0.794, x = [0.70467526 0.67284511 0.50217344 0.63171468 0.40092812]\n",
      "9. y = 0.792, x = [0.53134471 0.84669479 0.49451533 0.46530829 0.5732673 ]\n",
      "10. y = 0.785, x = [0.73573968 0.73870666 0.52359982 0.61458126 0.48539368]\n"
     ]
    }
   ],
   "source": [
    "q_title = \"Rick and Morty\"\n",
    "print_optimized_weights(q_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0c0db897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor, title, body, page_rank, page_views\n",
      "1. y = 0.751, x = [ 0.3802622   1.15174287  0.66534942  0.04572521 -0.02795971]\n",
      "2. y = 0.736, x = [0.55754871 1.25785412 0.54434859 0.09795087 0.38754463]\n",
      "3. y = 0.718, x = [ 0.70739086  1.01978448  0.34099951 -0.09697754  0.25905376]\n",
      "4. y = 0.712, x = [ 0.47024579  1.0151511   0.31649317 -0.00362412  0.52768426]\n",
      "5. y = 0.63, x = [0.50676697 1.00784559 0.3411947  0.0340439  0.65066275]\n",
      "6. y = 0.618, x = [0.44357598 1.0101824  0.14261672 0.0160672  0.88657244]\n",
      "7. y = 0.508, x = [0.5154044  1.01465605 0.57672439 0.05566338 0.09372452]\n",
      "8. y = 0.507, x = [ 0.52229925  0.92198258 -0.05646227  0.1800847   0.84241589]\n",
      "9. y = 0.491, x = [0.52162412 1.02846453 0.07928739 0.06320404 0.73917427]\n",
      "10. y = 0.373, x = [0.52621377 1.17490465 0.58755963 0.07533695 0.09622741]\n"
     ]
    }
   ],
   "source": [
    "q_title = \"Winter\"\n",
    "print_optimized_weights(q_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b58429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_rank_max ="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}