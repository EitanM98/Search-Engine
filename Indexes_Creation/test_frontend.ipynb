{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e402a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: nltk==3.7 in /opt/conda/miniconda3/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk==3.7) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk==3.7) (4.64.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk==3.7) (1.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk==3.7) (8.1.3)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: Flask in /opt/conda/miniconda3/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Flask) (2.2.2)\n",
      "Requirement already satisfied: click>=8.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Flask) (8.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Flask) (2.1.2)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Flask) (3.0.3)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Flask) (5.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=3.6.0->Flask) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Jinja2>=3.0->Flask) (2.1.1)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes\n",
    "!pip install nltk==3.7\n",
    "!pip install -U Flask\n",
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import hashlib\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "480dd9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = storage.Client()\n",
    "# bucket_name = 'ir_assg3_eithan'\n",
    "\n",
    "# bucket = client.bucket(bucket_name)\n",
    "\n",
    "# # Get a list of all the blobs in the folder\n",
    "# blobs = bucket.list_blobs(prefix='indexes/')\n",
    "\n",
    "# # Iterate over the list of blobs and download each one to a local file\n",
    "# for blob in blobs:\n",
    "#     if 'doc_norm_dict.pickle' in blob.name or 'doc_len_dict.pickle' in blob.name:\n",
    "#         local_file_name = blob.name[blob.name.rfind(\"/\")+1:]\n",
    "#         blob.download_to_filename(local_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5555d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/conda/miniconda3/lib/python3.8/site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from fasttext) (2.10.3)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from fasttext) (59.8.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.8/site-packages (from fasttext) (1.19.5)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: gensim in /opt/conda/miniconda3/lib/python3.8/site-packages (4.3.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/miniconda3/lib/python3.8/site-packages (from FuzzyTM>=0.4.0->gensim) (1.2.5)\n",
      "Requirement already satisfied: pyfume in /opt/conda/miniconda3/lib/python3.8/site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.6)\n",
      "Requirement already satisfied: fst-pso in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: simpful in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in /opt/conda/miniconda3/lib/python3.8/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in /opt/conda/miniconda3/lib/python3.8/site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n\u001B[1;32m     11\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpip install gensim\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwrappers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m \n",
      "File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/gensim/__init__.py:11\u001B[0m\n\u001B[1;32m      7\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m4.3.0\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001B[38;5;66;03m# noqa:F401\u001B[39;00m\n\u001B[1;32m     14\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgensim\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m logger\u001B[38;5;241m.\u001B[39mhandlers:  \u001B[38;5;66;03m# To ensure reload() doesn't add another one\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/gensim/corpora/__init__.py:6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mindexedcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m IndexedCorpus  \u001B[38;5;66;03m# noqa:F401 must appear before the other classes\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmmcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MmCorpus  \u001B[38;5;66;03m# noqa:F401\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbleicorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BleiCorpus  \u001B[38;5;66;03m# noqa:F401\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/gensim/corpora/indexedcorpus.py:14\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m interfaces, utils\n\u001B[1;32m     16\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mIndexedCorpus\u001B[39;00m(interfaces\u001B[38;5;241m.\u001B[39mCorpusABC):\n",
      "File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/gensim/interfaces.py:19\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m \n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils, matutils\n\u001B[1;32m     22\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mCorpusABC\u001B[39;00m(utils\u001B[38;5;241m.\u001B[39mSaveLoad):\n",
      "File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/gensim/matutils.py:1030\u001B[0m\n\u001B[1;32m   1025\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1.\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28mlen\u001B[39m(set1 \u001B[38;5;241m&\u001B[39m set2)) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mfloat\u001B[39m(union_cardinality)\n\u001B[1;32m   1028\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1029\u001B[0m     \u001B[38;5;66;03m# try to load fast, cythonized code if possible\u001B[39;00m\n\u001B[0;32m-> 1030\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_matutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001B[1;32m   1032\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m   1033\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlogsumexp\u001B[39m(x):\n",
      "File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/gensim/_matutils.pyx:1\u001B[0m, in \u001B[0;36minit gensim._matutils\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import collections\n",
    "import csv\n",
    "import gzip\n",
    "import math\n",
    "\n",
    "!pip install fasttext\n",
    "import fasttext\n",
    "import requests\n",
    "\n",
    "!pip install gensim\n",
    "from gensim.models.wrappers import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf43d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dataproc\n"
     ]
    }
   ],
   "source": [
    "%cd /home/dataproc\n",
    "# from inverted_index_gcp import InvertedIndex\n",
    "# from inverted_index_gcp import *\n",
    "# import inverted_index_gcp as iig\n",
    "from inverted_index_gcp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1aec54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(iig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe924cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_anchor = \"/home/dataproc/postings_gcp_anchor\"\n",
    "path_body = \"/home/dataproc/postings_gcp_body\"\n",
    "path_title = \"/home/dataproc/postings_gcp_title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf4405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read indexes\n",
    "body_index = InvertedIndex.read_index('.',\"body_index\")\n",
    "anchor_index = InvertedIndex.read_index('.',\"anchor_index\")\n",
    "title_index = InvertedIndex.read_index('.',\"title_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2edebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_index.bin_path = path_body\n",
    "anchor_index.bin_path = path_anchor\n",
    "title_index.bin_path = path_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ba0e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc_id_title_dict.pickle','rb') as f:\n",
    "    doc_title_dict = pickle.load(f)\n",
    "    \n",
    "with open('doc_len_dict.pickle','rb') as f:\n",
    "    doc_len_dict = pickle.load(f)\n",
    "    \n",
    "with open('page_views_dict.pkl','rb') as f:\n",
    "    page_views_dict = pickle.load(f)\n",
    "\n",
    "with open('page_rank_dict.pickle','rb') as f:\n",
    "    page_rank_dict = pickle.load(f)\n",
    "\n",
    "with open('doc_norm_dict.pickle','rb') as f:\n",
    "    doc_norm_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b092dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "def read_posting_list(inverted, w, index_type=\"\"):\n",
    "    with closing(MultiFileReader()) as reader:\n",
    "        locs = inverted.posting_locs[w]\n",
    "        b = reader.read(locs, inverted.df[w] * TUPLE_SIZE,index_type)\n",
    "        posting_list = []\n",
    "        for i in range(inverted.df[w]):\n",
    "            doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
    "            tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
    "            posting_list.append((doc_id, tf))\n",
    "        return posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e9281ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading posting list example:\n",
    "# lst = read_posting_list(body_index,\"texas\", path_body)\n",
    "# print(len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab5f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    tokens = set(tok for tok in tokens if tok not in all_stopwords)\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90c494f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_title():\n",
    "    ''' Returns ALL (not just top 100) search results that contain A QUERY WORD \n",
    "        IN THE TITLE of articles, ordered in descending order of the NUMBER OF \n",
    "        QUERY WORDS that appear in the title. For example, a document with a \n",
    "        title that matches two of the query words will be ranked before a \n",
    "        document with a title that matches only one query term. \n",
    "\n",
    "        Test this by navigating to the a URL like:\n",
    "         http://YOUR_SERVER_DOMAIN/search_title?query=hello+world\n",
    "        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n",
    "        if you're using ngrok on Colab or your external IP on GCP.\n",
    "    Returns:\n",
    "    --------\n",
    "        list of ALL (not just top 100) search results, ordered from best to \n",
    "        worst where each element is a tuple (wiki_id, title).\n",
    "    '''\n",
    "\n",
    "    # query=[\"hello world\"] - > [(wiki_id, title)] sorted decreasing\n",
    "    # in a way that the title that contains most words of the query\n",
    "\n",
    "    res = []\n",
    "#     query = request.args.get('hello world', '')\n",
    "    query = \"LinkedIn\"\n",
    "#     query = \"hello\"\n",
    "    if len(query) == 0:\n",
    "        return jsonify(res)\n",
    "    # BEGIN SOLUTION\n",
    "\n",
    "    counter = collections.Counter()\n",
    "    for token in tokenize(query):\n",
    "        for docId_tf in title_index.read_posting_list(token):\n",
    "            doc_id = docId_tf[0]\n",
    "            title = doc_title_dict[doc_id]\n",
    "            counter[(doc_id, title)] += 1\n",
    "\n",
    "    \n",
    "#     res = [map(lambda tup:tup[0], counter.most_common())] \n",
    "    res = list(map(lambda tup:tup[0], counter.most_common()))\n",
    "    # END SOLUTION\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b800ad24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(970755, 'LinkedIn'), (27769500, 'LinkedIn Pulse'), (36070366, '2012 LinkedIn hack'), (41726116, 'LinkedIn Learning'), (50191962, 'Timeline of LinkedIn'), (57147095, 'LinkedIn Top Companies'), (62976368, 'HiQ Labs v. LinkedIn')]\n"
     ]
    }
   ],
   "source": [
    "print(search_title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e881dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_anchor():\n",
    "    # query=[\"hello world\"] - > [(wiki_id, title)] sorted decreasing\n",
    "    # in a way that the title that contains most words of the query\n",
    "\n",
    "    res = []\n",
    "#     query = request.args.get('hello world', '')\n",
    "    query = \"LinkedIn\"\n",
    "#     query = \"hello\"\n",
    "    if len(query) == 0:\n",
    "        return jsonify(res)\n",
    "    # BEGIN SOLUTION\n",
    "\n",
    "    counter_anchor = collections.Counter()\n",
    "    for token in tokenize(query):\n",
    "        for docId_tf in anchor_index.read_posting_list(token):\n",
    "            doc_id = docId_tf[0]\n",
    "            title = doc_title_dict[doc_id]\n",
    "            counter_anchor[(doc_id, title)] += 1\n",
    "\n",
    "    \n",
    "#     res = [map(lambda tup:tup[0], counter.most_common())] \n",
    "    res = list(map(lambda tup:tup[0], counter_anchor.most_common()))\n",
    "    # END SOLUTION\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a58d58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(search_anchor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95b06982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pageview():\n",
    "    ''' Returns the number of page views that each of the provide wiki articles\n",
    "        had in August 2021.\n",
    "\n",
    "        Test this by issuing a POST request to a URL like:\n",
    "          http://YOUR_SERVER_DOMAIN/get_pageview\n",
    "        with a json payload of the list of article ids. In python do:\n",
    "          import requests\n",
    "          requests.post('http://YOUR_SERVER_DOMAIN/get_pageview', json=[1,5,8])\n",
    "        As before YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n",
    "        if you're using ngrok on Colab or your external IP on GCP.\n",
    "    Returns:\n",
    "    --------\n",
    "        list of ints:\n",
    "          list of page view numbers from August 2021 that correrspond to the \n",
    "          provided list article IDs.\n",
    "    '''\n",
    "\n",
    "    res = []\n",
    "#     wiki_ids = request.get_json()\n",
    "    wiki_ids = [848, 21105323, 23536538, 23797577]\n",
    "    if len(wiki_ids) == 0:\n",
    "        return res\n",
    "    # BEGIN SOLUTION\n",
    "    \n",
    "    res = [page_views_dict.get(doc_id, -1) for doc_id in wiki_ids]\n",
    "\n",
    "    # END SOLUTION\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be6bd398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84370, 1042, 30483, 38197]\n"
     ]
    }
   ],
   "source": [
    "print(get_pageview())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "007f8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagerank():\n",
    "    ''' Returns PageRank values for a list of provided wiki article IDs. \n",
    "\n",
    "        Test this by issuing a POST request to a URL like:\n",
    "          http://YOUR_SERVER_DOMAIN/get_pagerank\n",
    "        with a json payload of the list of article ids. In python do:\n",
    "          import requests\n",
    "          requests.post('http://YOUR_SERVER_DOMAIN/get_pagerank', json=[1,5,8])\n",
    "        As before YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n",
    "        if you're using ngrok on Colab or your external IP on GCP.\n",
    "    Returns:\n",
    "    --------\n",
    "        list of floats:\n",
    "          list of PageRank scores that correrspond to the provided article IDs.\n",
    "    '''\n",
    "    res = []\n",
    "#     wiki_ids = request.get_json()\n",
    "    wiki_ids = [848, 3434750, 32927]\n",
    "    if len(wiki_ids) == 0:\n",
    "        return jsonify(res)\n",
    "    # BEGIN SOLUTION\n",
    "\n",
    "    res = [page_rank_dict.get(doc_id, -1) for doc_id in wiki_ids]\n",
    "    # END SOLUTION\n",
    "#     return jsonify(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bad9d36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50.7020900020747, 9913.728782160773, 5282.081575765278]\n"
     ]
    }
   ],
   "source": [
    "print(get_pagerank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44502b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "N = 6348910\n",
    "\n",
    "def normalize(tokens):\n",
    "    counter = Counter(tokens)\n",
    "    norm = 0\n",
    "    for value in counter.values():\n",
    "        norm += value*value\n",
    "    if norm == 0 :\n",
    "        return 0\n",
    "    return 1/(math.sqrt(norm))\n",
    "\n",
    "\n",
    "def tf_idf_calc (token, doc_id, tf):\n",
    "    tf =  tf/doc_len_dict[doc_id]\n",
    "#     idf = math.log(N/app.body_index.df[token], 2)\n",
    "    idf = math.log(N/body_index.df[token], 2)\n",
    "    return tf*idf\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    tokens = [tok for tok in tokens if tok not in all_stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ade5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_body():\n",
    "    ''' Returns up to a 100 search results for the query using TFIDF AND COSINE\n",
    "        SIMILARITY OF THE BODY OF ARTICLES ONLY. DO NOT use stemming. DO USE the \n",
    "        staff-provided tokenizer from Assignment 3 (GCP part) to do the \n",
    "        tokenization and remove stopwords. \n",
    "\n",
    "        To issue a query navigate to a URL like:\n",
    "         http://YOUR_SERVER_DOMAIN/search_body?query=hello+world\n",
    "        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n",
    "        if you're using ngrok on Colab or your external IP on GCP.\n",
    "    Returns:\n",
    "    --------\n",
    "        list of up to 100 search results, ordered from best to worst where each \n",
    "        element is a tuple (wiki_id, title).\n",
    "    '''\n",
    "    res = []\n",
    "#     query = request.args.get('query', '')\n",
    "    query = 'apple computer'\n",
    "    if len(query) == 0:\n",
    "        return jsonify(res)\n",
    "    # BEGIN SOLUTION\n",
    "\n",
    "    similarity_dict = {}\n",
    "    tokens = tokenize(query)\n",
    "    for token in tokens:\n",
    "        for doc_tf in body_index.read_posting_list(token):\n",
    "            if doc_tf[0] not in similarity_dict:\n",
    "                similarity_dict[doc_tf[0]] = 0\n",
    "            similarity_dict[doc_tf[0]] += tf_idf_calc(token, doc_tf[0], doc_tf[1])\n",
    "\n",
    "    for doc in similarity_dict.keys():\n",
    "        similarity_dict[doc] = similarity_dict[doc]*normalize(tokens)*doc_norm_dict[doc]\n",
    "        \n",
    "    top100 = list(sorted(similarity_dict.items(), key=lambda item: item[1], reverse=True)[:100])\n",
    "    \n",
    "    for pair in top100:\n",
    "        res.append((pair[0], doc_title_dict[pair[0]]))\n",
    "\n",
    "    # END SOLUTION\n",
    "#     return jsonify(res)\n",
    "#     return res\n",
    "    return top100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b72f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(search_body())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe1fe094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fastText'...\n",
      "remote: Enumerating objects: 3930, done.\u001B[K\n",
      "remote: Counting objects: 100% (944/944), done.\u001B[K\n",
      "remote: Compressing objects: 100% (140/140), done.\u001B[K\n",
      "remote: Total 3930 (delta 854), reused 804 (delta 804), pack-reused 2986\u001B[K\n",
      "Receiving objects: 100% (3930/3930), 8.24 MiB | 36.07 MiB/s, done.\n",
      "Resolving deltas: 100% (2505/2505), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/fastText.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54f362be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "410b3087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/conda/miniconda3/lib/python3.8/site-packages (0.9.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from fasttext) (59.8.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.8/site-packages (from fasttext) (1.19.5)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from fasttext) (2.10.3)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5df3a46",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# import fasttext\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwrappers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FastText \n\u001B[1;32m      4\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwiki.en\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# fasttext.download_model(model_name, if_exists='ignore')\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# import fasttext\n",
    "\n",
    "\n",
    "model_name = \"wiki.en\"\n",
    "# fasttext.download_model(model_name, if_exists='ignore')\n",
    "\n",
    "fasttext.download_model(model_name, if_exists='ignore',path='/home/dataproc/fasttext_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de15008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia 2017 trained Fasttext model \n",
    "model_url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec'\n",
    "response = requests.get(model_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66332f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-10 14:53:37--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6597238061 (6.1G) [binary/octet-stream]\n",
      "Saving to: ‘wiki.en.vec’\n",
      "\n",
      "wiki.en.vec         100%[===================>]   6.14G  53.0MB/s    in 2m 10s  \n",
      "\n",
      "2023-01-10 14:55:48 (48.4 MB/s) - ‘wiki.en.vec’ saved [6597238061/6597238061]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ca806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model to a binary file\n",
    "# for chunk in response.iter_content(chunk_size=1024):\n",
    "#     if chunk: # filter out keep-alive new chunks\n",
    "#         f.write(chunk)\n",
    "\n",
    "# with open(\"fasttext_model.bin\", \"wb\") as f:\n",
    "#     f.write(response.content)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8dd5d584",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_model() got an unexpected keyword argument 'format'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Load the model from the binary file\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# model = fasttext.load_model(\"wiki.en.vec\")\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# model = fasttext.load_model('wiki.en.vec')\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mfasttext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwiki.en.vec\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvec\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: load_model() got an unexpected keyword argument 'format'"
     ]
    }
   ],
   "source": [
    "# Load the model from the binary file\n",
    "# model = fasttext.load_model(\"wiki.en.vec\")\n",
    "# model = fasttext.load_model('wiki.en.vec')\n",
    "model = fasttext.load_model('wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "333abe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "wiki.en.vec has wrong file format!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Load the model from the binary file\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mfasttext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwiki.en.vec\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Use the model to get the vector representation of a word\u001B[39;00m\n\u001B[1;32m      5\u001B[0m vector \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mget_word_vector(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhello\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/fasttext/FastText.py:441\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;124;03m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001B[39;00m\n\u001B[1;32m    440\u001B[0m eprint(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWarning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 441\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_FastText\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/fasttext/FastText.py:98\u001B[0m, in \u001B[0;36m_FastText.__init__\u001B[0;34m(self, model_path, args)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;241m=\u001B[39m fasttext\u001B[38;5;241m.\u001B[39mfasttext()\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 98\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloadModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: wiki.en.vec has wrong file format!"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the model to get the vector representation of a word\n",
    "vector = model.get_word_vector('hello')\n",
    "\n",
    "# Use the model to get the most similar words to a given word\n",
    "similar_words = model.get_nearest_neighbors('hello')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d6a8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}